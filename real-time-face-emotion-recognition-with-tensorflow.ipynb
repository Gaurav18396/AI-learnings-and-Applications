{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd6efa2-5f4f-4dc1-b626-a0564f7baf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Project Report for Artificail intelligence 2:\n",
    "    Gaurav Dongare\n",
    "    Vinay Pawar\n",
    "    \n",
    "Under the Guidance of:\n",
    "    Prof. Shreeganesh Thottempudi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f62fd28-0010-48a0-b001-127877213b1d",
   "metadata": {},
   "source": [
    "Real Time Face Emotion Recognition using Deep Learning Model: CNN - Convolution Neural Network.\n",
    "\n",
    "Topics:\n",
    "1) Introduction\n",
    "2) CNN History\n",
    "3) CNN Architecture\n",
    "4) CNN Use Case\n",
    "\n",
    "\n",
    "Introduction:\n",
    "\n",
    "Convolutional Neural Networks (CNNs) have revolutionized the field of computer vision and pattern recognition. This report provides a comprehensive overview of CNNs, it's architecture and applications. The report explains the underlying principles of CNNs, their layers, and how they are utilized for tasks such as image classification, object detection, and image segmentation.\n",
    "\n",
    "What is CNN?\n",
    "\n",
    "    A Convolutional Neural Network (CNN) is like a smart tool that looks at pictures or words and figures out the most important parts. It changes the information in a way that makes it faster to work with, and it's good at handling mistakes and messy stuff. CNNs are really good at understanding images and texts, and they're often used to spot objects in pictures or find specific parts of texts. CNN stands for Convolution Neural Network. What's cool about CNNs is that they can do many things at once, they're good at sharing knowledge, and they learn step by step to understand things better. These features make CNNs even more interesting and useful.\n",
    "\n",
    "\n",
    "CNN History:\n",
    "\n",
    "    The history of Convolutional Neural Networks (CNNs) is a journey of innovation in the world of artificial intelligence. Starting in the late 20th century, CNNs were initially inspired by the human visual system. In the 1980s, Yann LeCun introduced one of the earliest forms of CNNs called LeNet-5, which paved the way for image recognition. However, due to limited computational resources, CNNs didn't gain much popularity until the 2010s when the explosion of data and computing power allowed for their practical implementation. This era saw the rise of deep CNNs with remarkable achievements, like AlexNet winning the ImageNet competition in 2012. Further developments, such as VGGNet, GoogLeNet, and ResNet, continued to enhance CNN performance, enabling breakthroughs in image classification, object detection, and more. As of today, CNNs remain a cornerstone of modern AI, with their applications spanning across industries and revolutionizing the field of computer vision.\n",
    "\n",
    "CNN Architecture:\n",
    "At the heart of a CNN, you have layers that do different tasks:\n",
    "\n",
    "Input Layer:\n",
    "This is where you feed in the image you want the CNN to understand.\n",
    "\n",
    "Convolutional Layers:\n",
    "These layers use special filters to find specific patterns in the image, like edges, corners, or textures. Imagine you're looking at a puzzle piece by piece.\n",
    "\n",
    "Activation Layers:\n",
    "After finding patterns, the network decides if they matter. It's like telling the network, \"Hey, this part is important, and this part is not.\"\n",
    "\n",
    "Pooling Layers:\n",
    "These layers shrink down the picture by keeping only the main information. Imagine you're looking at the bigger picture instead of every tiny detail.\n",
    "\n",
    "Fully Connected Layers:\n",
    "These layers understand what patterns the network found and make a guess about what the image is showing. It's like the network saying, \"Based on all these things I've seen, I think this picture is a cat!\"\n",
    "\n",
    "Output Layer:\n",
    "The final answer is given here. The network tells you its best guess about what's in the picture.\n",
    "\n",
    "CNNs work by repeating these layers in a deep structure, learning more and more about the image as they go along. This architecture is designed to capture both simple and complex features, helping the network recognize intricate patterns in images.\n",
    "\n",
    "\n",
    "CNN Use Case:\n",
    "\n",
    "Following is the use cases taken from Kaggle. The use case is used as a reference to understand the architecture of CNN and its application in real time. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69043616-515f-4b76-b968-0bb63c643e65",
   "metadata": {},
   "source": [
    "The code begins by importing the TensorFlow library, which is a widely used open-source machine learning framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46106c43",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 16.051339,
     "end_time": "2023-05-31T17:19:52.268185",
     "exception": false,
     "start_time": "2023-05-31T17:19:36.216846",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866784da-6edb-4ffb-9884-9606731bb16f",
   "metadata": {},
   "source": [
    "The paths to the directories containing training and testing data are specified here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6de94848",
   "metadata": {
    "papermill": {
     "duration": 0.010771,
     "end_time": "2023-05-31T17:19:52.283137",
     "exception": false,
     "start_time": "2023-05-31T17:19:52.272366",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dir = \"train\"\n",
    "test_dir = \"test\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56892381-de00-4120-b55b-4329ca2957fe",
   "metadata": {},
   "source": [
    "1) Data Preprocessing:\n",
    "ImageDataGenerator is object from TensorFlow's Keras module. This object is used to preprocess and augment image data. The options provided include:\n",
    "\n",
    "    rescale=1./255: This rescales the pixel values of the images to be between 0 and 1, which is a common preprocessing step.\n",
    "\n",
    "    horizontal_flip=True: It horizontally flips the images, which helps in augmenting the dataset and making the model more robust.\n",
    "\n",
    "    validation_split=0.2: It sets aside 20% of the data for validation during training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79937566",
   "metadata": {
    "papermill": {
     "duration": 4.875435,
     "end_time": "2023-05-31T17:19:57.161680",
     "exception": false,
     "start_time": "2023-05-31T17:19:52.286245",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataGenerator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255, horizontal_flip=True, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295b1e10-7b41-4c7d-93fc-381f7296292d",
   "metadata": {},
   "source": [
    "2) Generating Trainign Data and Validation data:\n",
    "\n",
    "    train_dir is the directory path where the training images are stored.\n",
    "    \n",
    "    batch_size=64: It means that 64 images will be processed in each iteration.\n",
    "    \n",
    "    target_size=(48, 48): The images are resized to 48x48 pixels.\n",
    "    \n",
    "    shuffle=True: The data is shuffled randomly.\n",
    "    \n",
    "    color_mode='grayscale': Images are converted to grayscale.\n",
    "    \n",
    "    class_mode='categorical': The labels are categorical (one-hot encoded) indicating different classes.\n",
    "    \n",
    "    subset='training': It specifies that this data generator will be used for training.\n",
    "\n",
    "    subset='validation': It indicates that this data generator will be used for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37fd163c-4e60-4c22-a3ba-e1ec7576e1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22968 images belonging to 7 classes.\n",
      "Found 5741 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "training_data = dataGenerator.flow_from_directory(train_dir, batch_size=64, target_size=(48, 48), shuffle=True, color_mode='grayscale', class_mode='categorical', subset='training')\n",
    "validation_set = dataGenerator.flow_from_directory(train_dir, batch_size=64, target_size=(48, 48), shuffle=True, color_mode='grayscale', class_mode='categorical', subset='validation')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dba5c6-46df-419c-a013-4d16f265ba3f",
   "metadata": {},
   "source": [
    "3) Genrating Test Data:\n",
    "\n",
    "    test_dir is the directory path where the test images are stored.\n",
    "    \n",
    "    The testDataGenerator object is similar to the one used for training and validation, with rescaling and horizontal flipping.\n",
    "    \n",
    "    The test data generator doesn't have the subset parameter because all test data is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46064051-c0e0-4ecf-bcbd-318f9627bfdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7178 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "testDataGenerator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255, horizontal_flip=True)\n",
    "\n",
    "test_data = testDataGenerator.flow_from_directory(test_dir, batch_size=64, target_size=(48, 48), shuffle=True, color_mode='grayscale', class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27604a15-fffe-4185-be06-45acec433eca",
   "metadata": {},
   "source": [
    "4) Defining a function to create model:\n",
    "\n",
    "    The create_model function defines a CNN architecture with multiple convolutional and pooling layers, dropout for regularization, and fully connected layers for classification. These layers are structured to learn and extract features from input images, gradually capturing more complex patterns as the network gets deeper.\n",
    "    \n",
    "    weight_decay is a parameter used to control regularization, which helps prevent overfitting. A Sequential model is created, which is a linear stack of layers.\n",
    "    \n",
    "    Conv2D layers perform convolutions on the input images. Here, a 64 filters of size (4, 4) are applied. padding='same' ensures the output size is the same as the input size. \n",
    "    \n",
    "    Activation('relu') applies the ReLU activation function to the output of the convolution.\n",
    "    \n",
    "    BatchNormalization() helps normalize the output, making training more stable and faster.\n",
    "    \n",
    "    MaxPool2D layers perform max pooling, reducing the spatial dimensions of the data while keeping important features.\n",
    "    \n",
    "    Dropout(0.2) randomly \"drops out\" a fraction of neurons during training to prevent overfitting.\n",
    "    \n",
    "It is repeated three times with different settings for varying complexity and abstraction in the features learned.\n",
    "\n",
    "    Flatten layer reshapes the output into a 1D vector.\n",
    "    \n",
    "    Dense layers are fully connected layers, where each neuron is connected to every neuron in the previous layer.\n",
    "    \n",
    "    The final layer has 7 neurons and uses softmax activation to predict the probability of each of the 7 classes (emotions).\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cfe43ff",
   "metadata": {
    "papermill": {
     "duration": 0.021326,
     "end_time": "2023-05-31T17:19:57.186893",
     "exception": false,
     "start_time": "2023-05-31T17:19:57.165567",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    weight_decay = 1e-4\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(64, (4, 4), padding='same', kernel_regularizer=tf.keras.regularizers.l2(weight_decay), input_shape=(48, 48, 1)))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Conv2D(64, (4, 4), padding='same', kernel_regularizer=tf.keras.regularizers.l2(weight_decay)))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2)))\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(128, (4, 4), padding='same', kernel_regularizer=tf.keras.regularizers.l2(weight_decay)))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2)))\n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(128, (4, 4), padding='same', kernel_regularizer=tf.keras.regularizers.l2(weight_decay)))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Conv2D(128, (4, 4), padding='same', kernel_regularizer=tf.keras.regularizers.l2(weight_decay)))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2)))\n",
    "    model.add(tf.keras.layers.Dropout(0.4))\n",
    "    \n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(128, activation=\"linear\"))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.Dense(7, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48619a84-2e25-4f16-897c-2f2a4f2ce3a8",
   "metadata": {},
   "source": [
    "5) Creating Model\n",
    "    This below line of code calls the previously defined create_model function to instantiate the CNN model. It means that the architecture described in the create_model function will be used to create the actual neural network.\n",
    "\n",
    "    compile is a method used to configure the learning process of the model.\n",
    "\n",
    "    loss='categorical_crossentropy' specifies the loss function to measure how well the model's predictions match the actual labels.\n",
    "\n",
    "    optimizer=tf.keras.optimizers.Adam(0.0003) sets the optimizer used to adjust the model's parameters during training. Here, Adam optimizer with a learning rate of 0.0003 is chosen.\n",
    "\n",
    "    metrics=['accuracy'] defines the metric to evaluate the model's performance during training. In this case, accuracy (the proportion of correctly predicted cases) is chosen as the metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ec071b0",
   "metadata": {
    "papermill": {
     "duration": 6.139756,
     "end_time": "2023-05-31T17:20:03.329975",
     "exception": false,
     "start_time": "2023-05-31T17:19:57.190219",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(0.0003), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7075243e-b03f-4671-9b38-59cb85ce7782",
   "metadata": {},
   "source": [
    "The checkpointer is a list of two callbacks: \n",
    "EarlyStopping stops training when the validation accuracy plateaus, and ModelCheckpoint saves the best model weights during training. These callbacks enhance the training process, preventing overfitting and allowing you to restore the best-performing model.\n",
    "\n",
    "In Below code following are the attributes:\n",
    "\n",
    "EarlyStopping and is a callback that monitors a specific metric during training and stops the training process if that metric stops improving.\n",
    "\n",
    "monitor='val_accuracy' indicates that the validation accuracy will be monitored.\n",
    "\n",
    "verbose=1 means that messages will be printed to the console to provide information about the training process.\n",
    "\n",
    "restore_best_weights=True restores the model's weights to the best state when training is stopped.\n",
    "\n",
    "mode=\"max\" specifies that the metric should be maximized (in this case, validation accuracy).\n",
    "\n",
    "patience=10 means that if the validation accuracy doesn't improve for 10 consecutive epochs, the training will be stopped.\n",
    "\n",
    "\n",
    "ModelCheckpoint is a callback that saves the model's weights during training.\n",
    "\n",
    "filepath='final_model_weights.hdf5' specifies the file path where the best model weights will be saved.\n",
    "\n",
    "monitor=\"val_accuracy\" means that the validation accuracy will be monitored to decide when to save the weights.\n",
    "\n",
    "verbose=1 provides information about the saving process.\n",
    "\n",
    "save_best_only=True indicates that only the weights associated with the best validation accuracy will be saved.\n",
    "\n",
    "mode=\"max\" specifies that the metric (validation accuracy) should be maximized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cec1ee45",
   "metadata": {
    "papermill": {
     "duration": 0.012045,
     "end_time": "2023-05-31T17:20:03.345930",
     "exception": false,
     "start_time": "2023-05-31T17:20:03.333885",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpointer = [tf.keras.callbacks.EarlyStopping(monitor = 'val_accuracy', verbose = 1, restore_best_weights=True, mode=\"max\",patience = 10),\n",
    "                tf.keras.callbacks.ModelCheckpoint(\n",
    "                    filepath='final_model_weights.hdf5',\n",
    "                    monitor=\"val_accuracy\",\n",
    "                    verbose=1,\n",
    "                    save_best_only=True,\n",
    "                    mode=\"max\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4e2698-6a7f-4281-b04e-c43eccd72158",
   "metadata": {},
   "source": [
    "6) Training Model\n",
    "\n",
    "    The Below code trains the model using the training data and validates its performance using the validation data. The number of steps and epochs is controlled, and the defined callbacks are utilized to enhance the training process and monitor the model's progress.\n",
    "\n",
    "    model.fit() trains the model using the provided data.\n",
    "\n",
    "    x=training_data specifies the training data to be used.\n",
    "\n",
    "    validation_data=validation_set indicates the validation data to be used to check the model's performance.\n",
    "\n",
    "    epochs=10 sets the number of times the model will go through the entire training dataset.\n",
    "\n",
    "    callbacks=[checkpointer] applies the callbacks previously defined, such as early stopping and model checkpointing.\n",
    "\n",
    "    steps_per_epoch=steps_per_epoch determines how many batches of data are processed in each epoch..\n",
    "\n",
    "    validation_steps=validation_steps sets the number of batches used for validation in each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8c8a26b",
   "metadata": {
    "papermill": {
     "duration": 697.447188,
     "end_time": "2023-05-31T17:31:40.796438",
     "exception": false,
     "start_time": "2023-05-31T17:20:03.349250",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "358/358 [==============================] - ETA: 0s - loss: 1.7922 - accuracy: 0.3172\n",
      "Epoch 1: val_accuracy improved from -inf to 0.26650, saving model to final_model_weights.hdf5\n",
      "358/358 [==============================] - 1133s 3s/step - loss: 1.7922 - accuracy: 0.3172 - val_loss: 1.8294 - val_accuracy: 0.2665\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gaura\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "358/358 [==============================] - ETA: 0s - loss: 1.5574 - accuracy: 0.4067\n",
      "Epoch 2: val_accuracy improved from 0.26650 to 0.42591, saving model to final_model_weights.hdf5\n",
      "358/358 [==============================] - 477s 1s/step - loss: 1.5574 - accuracy: 0.4067 - val_loss: 1.5183 - val_accuracy: 0.4259\n",
      "Epoch 3/10\n",
      "358/358 [==============================] - ETA: 0s - loss: 1.4313 - accuracy: 0.4605\n",
      "Epoch 3: val_accuracy improved from 0.42591 to 0.47946, saving model to final_model_weights.hdf5\n",
      "358/358 [==============================] - 550s 2s/step - loss: 1.4313 - accuracy: 0.4605 - val_loss: 1.3712 - val_accuracy: 0.4795\n",
      "Epoch 4/10\n",
      "358/358 [==============================] - ETA: 0s - loss: 1.3352 - accuracy: 0.5024\n",
      "Epoch 4: val_accuracy improved from 0.47946 to 0.50878, saving model to final_model_weights.hdf5\n",
      "358/358 [==============================] - 376s 1s/step - loss: 1.3352 - accuracy: 0.5024 - val_loss: 1.3356 - val_accuracy: 0.5088\n",
      "Epoch 5/10\n",
      "358/358 [==============================] - ETA: 0s - loss: 1.2622 - accuracy: 0.5305\n",
      "Epoch 5: val_accuracy improved from 0.50878 to 0.53178, saving model to final_model_weights.hdf5\n",
      "358/358 [==============================] - 317s 886ms/step - loss: 1.2622 - accuracy: 0.5305 - val_loss: 1.2734 - val_accuracy: 0.5318\n",
      "Epoch 6/10\n",
      "358/358 [==============================] - ETA: 0s - loss: 1.2154 - accuracy: 0.5549\n",
      "Epoch 6: val_accuracy improved from 0.53178 to 0.53652, saving model to final_model_weights.hdf5\n",
      "358/358 [==============================] - 370s 1s/step - loss: 1.2154 - accuracy: 0.5549 - val_loss: 1.2478 - val_accuracy: 0.5365\n",
      "Epoch 7/10\n",
      "358/358 [==============================] - ETA: 0s - loss: 1.1587 - accuracy: 0.5780\n",
      "Epoch 7: val_accuracy improved from 0.53652 to 0.54951, saving model to final_model_weights.hdf5\n",
      "358/358 [==============================] - 427s 1s/step - loss: 1.1587 - accuracy: 0.5780 - val_loss: 1.2342 - val_accuracy: 0.5495\n",
      "Epoch 8/10\n",
      "358/358 [==============================] - ETA: 0s - loss: 1.1298 - accuracy: 0.5928\n",
      "Epoch 8: val_accuracy improved from 0.54951 to 0.57128, saving model to final_model_weights.hdf5\n",
      "358/358 [==============================] - 419s 1s/step - loss: 1.1298 - accuracy: 0.5928 - val_loss: 1.1819 - val_accuracy: 0.5713\n",
      "Epoch 9/10\n",
      "358/358 [==============================] - ETA: 0s - loss: 1.0939 - accuracy: 0.6059\n",
      "Epoch 9: val_accuracy improved from 0.57128 to 0.58445, saving model to final_model_weights.hdf5\n",
      "358/358 [==============================] - 356s 992ms/step - loss: 1.0939 - accuracy: 0.6059 - val_loss: 1.1675 - val_accuracy: 0.5844\n",
      "Epoch 10/10\n",
      "358/358 [==============================] - ETA: 0s - loss: 1.0673 - accuracy: 0.6133\n",
      "Epoch 10: val_accuracy did not improve from 0.58445\n",
      "358/358 [==============================] - 338s 943ms/step - loss: 1.0673 - accuracy: 0.6133 - val_loss: 1.1656 - val_accuracy: 0.5776\n"
     ]
    }
   ],
   "source": [
    "steps_per_epoch = training_data.n // training_data.batch_size\n",
    "validation_steps = validation_set.n // validation_set.batch_size\n",
    "#These lines calculate the number of steps needed for each epoch of training and validation. \n",
    "#It's based on the number of samples in the dataset divided by the batch size.\n",
    "\n",
    "history = model.fit(x=training_data,\n",
    "                 validation_data=validation_set,\n",
    "                 epochs=10,\n",
    "                 callbacks=[checkpointer],\n",
    "                 steps_per_epoch=steps_per_epoch,\n",
    "                 validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d28dc1-ddc0-4e2e-9599-abb0873cda7a",
   "metadata": {},
   "source": [
    "Below code  calculates and displays the accuracy of the trained model on the test dataset, giving an indication of how well the model generalizes to new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70675d5",
   "metadata": {
    "papermill": {
     "duration": 19.980027,
     "end_time": "2023-05-31T17:32:01.166768",
     "exception": false,
     "start_time": "2023-05-31T17:31:41.186741",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 55s 493ms/step - loss: 1.1485 - accuracy: 0.5804\n",
      "Test accuracy = 58.03571343421936%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test accuracy = {model.evaluate(test_data ,batch_size=test_data.batch_size,steps=test_data.n // test_data.batch_size)[1]*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6404a2-9b68-4e39-a8c1-4b008585197f",
   "metadata": {},
   "source": [
    " This code captures real-time webcam frames, detects faces, resizes and preprocesses face images, uses the trained CNN model to predict emotions, and overlays rectangles and labels on the frames to show the recognized emotions. The code provides an interactive way to visualize the model's performance in real time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d698762",
   "metadata": {
    "papermill": {
     "duration": 0.356752,
     "end_time": "2023-05-31T17:32:01.876965",
     "exception": false,
     "start_time": "2023-05-31T17:32:01.520213",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Code for real time prediction \n",
    "\n",
    "'''\n",
    "The code begins by importing the necessary libraries: cv2 for image processing, tensorflow for using the trained model, \n",
    "and numpy for numerical operations.\n",
    "class_names is a list of emotion labels that the model can predict.\n",
    "The trained model is loaded using tf.keras.models.load_model().\n",
    "'''\n",
    "\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class_names = [\"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Neutral\", \"Sad\", \"Surprise\"]\n",
    "\n",
    "model = tf.keras.models.load_model('final_model_weights.hdf5')\n",
    "\n",
    "\n",
    "'''\n",
    "The Below code initializes a video capture object to access the webcam feed (0 indicates the default camera).\n",
    "faceDetect is an instance of CascadeClassifier from OpenCV, which is used for face detection.\n",
    "'''\n",
    "\n",
    "video = cv2.VideoCapture(0)\n",
    "\n",
    "faceDetect = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "'''\n",
    "This loop captures frames from the video feed and converts them to grayscale.\n",
    "The detectMultiScale function detects faces in the grayscale frame and returns their coordinates.\n",
    "'''\n",
    "\n",
    "while True:\n",
    "    ret, frame = video.read()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = faceDetect.detectMultiScale(gray, 1.3, 3)\n",
    "    \n",
    "'''\n",
    "This loop iterates through each detected face:\n",
    "A sub-image of the face region is extracted and resized to 48x48 pixels.\n",
    "The pixel values are normalized between 0 and 1.\n",
    "The normalized image is reshaped to match the model's input shape.\n",
    "The model predicts the emotion label for the resized face image.\n",
    "The label variable stores the index of the predicted emotion.\n",
    "'''\n",
    "\n",
    "    for x, y, w, h in faces:\n",
    "        sub_face_img = gray[y : y + h, x : x + w]\n",
    "        resized = cv2.resize(sub_face_img, (48, 48))\n",
    "        normalize = resized / 255.0\n",
    "        reshaped = np.reshape(normalize, (1, 48, 48, 1))\n",
    "        result = model.predict(reshaped)\n",
    "        label = np.argmax(result, axis=1)[0]\n",
    "        print(label)\n",
    "        \n",
    "'''\n",
    "This part draws rectangles around the detected faces and labels them with predicted emotions:\n",
    "The rectangles are drawn using the cv2.rectangle function.\n",
    "The cv2.putText function adds the predicted emotion label on top of the rectangle.\n",
    "'''\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 0, 255), 1)\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (50, 50, 255), 2)\n",
    "        cv2.rectangle(frame, (x, y - 40), (x + w, y), (50, 50, 255), -1)\n",
    "        cv2.putText(frame, class_names[label], (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "'''\n",
    "The processed frame with rectangles and labels is displayed in a window using cv2.imshow.\n",
    "The loop continues until the user presses the 'q' key, at which point the video capture is released and the windows are closed.\n",
    "'''\n",
    "\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    k = cv2.waitKey(1)\n",
    "    if k == ord('q'):\n",
    "        break\n",
    "\n",
    "video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db8dd50-c234-4915-99d7-c35d1f61c940",
   "metadata": {},
   "source": [
    "In conclusion, the project involves the implementation of a real-time emotion recognition system using a Convolutional Neural Network (CNN). The primary goal of the project is to accurately detect and label emotions in faces captured through a webcam feed. The project integrates various components, including data preprocessing, model architecture design, training, and real-time inference.\n",
    "\n",
    "The key components of the project are as follows:\n",
    "\n",
    "Data Preprocessing:\n",
    "The project begins by preprocessing image data using TensorFlow's ImageDataGenerator. The images are rescaled to a common range, augmented with horizontal flips, and split into training and validation sets. This preprocessing enhances the model's ability to generalize and perform well on new, unseen data.\n",
    "\n",
    "CNN Model Architecture:\n",
    "The CNN model architecture is designed to capture meaningful features from face images. It comprises multiple convolutional layers for feature extraction, activation functions to introduce non-linearity, batch normalization to stabilize training, max pooling for spatial reduction, and dropout layers for regularization. The model culminates with fully connected layers for classification into emotion categories.\n",
    "\n",
    "Training and Evaluation:\n",
    "The model is compiled with appropriate loss, optimizer, and metrics settings. It is then trained using the prepared training data, with callbacks such as EarlyStopping and ModelCheckpoint to prevent overfitting and save the best model weights. After training, the model's accuracy and performance are evaluated on a separate test dataset.\n",
    "\n",
    "Real-Time Emotion Recognition:\n",
    "To demonstrate the model's capability, a real-time emotion recognition system is developed using OpenCV for webcam access. The system detects faces, preprocesses them, feeds them into the trained model, and overlays emotion labels on the frames. This interactive application provides a practical way to showcase the model's performance in real-world scenarios.\n",
    "\n",
    "In essence, the project successfully achieves the goal of creating an end-to-end emotion recognition system. By combining deep learning techniques with real-time video processing, the system is able to predict and display emotions in real time. This project serves as an example of how CNNs can be applied to real-world applications, showcasing their power in understanding and interpreting visual data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775973e5-b9a7-4dfb-8c46-782b0b36d170",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "Bhatt, D.; Patel, C.; Talsania,\n",
    "H.; Patel, J.; Vaghela, R.; Pandya, S.;\n",
    "Modi, K.; Ghayvat, H. CNN Variants\n",
    "for Computer Vision: History,\n",
    "Architecture, Application, Challenges\n",
    "and Future Scope. Electronics 2021, 10,\n",
    "2470. https://doi.org/10.3390/\n",
    "electronics10202470\n",
    "\n",
    "https://www.kaggle.com/code/mihaililie/real-time-face-emotion-recognition-with-tensorflow\n",
    "\n",
    "https://www.ibm.com/topics/convolutional-neural-networks\n",
    "\n",
    "https://saturncloud.io/blog/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way/\n",
    "\n",
    "https://medium.com/analytics-vidhya/building-a-real-time-emotion-detector-towards-machine-with-e-q-c20b17f89220"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 763.969298,
   "end_time": "2023-05-31T17:32:05.626181",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-05-31T17:19:21.656883",
   "version": "2.4.0"
  },
  "toc-autonumbering": true,
  "toc-showcode": true,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
